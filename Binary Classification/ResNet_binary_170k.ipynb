{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries for deep learning, metrics, image processing, and multiprocessing\n",
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, DistributedSampler, SubsetRandomSampler\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Custom dataset class for loading jet images and their labels\n",
    "class JetImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir # Directory where images are stored\n",
    "        self.transform = transform # Image transformations to be applied\n",
    "        self.images = []  # List to hold image filenames\n",
    "        self.labels = []  # List to hold labels\n",
    "\n",
    "        # Traverse the directory and label each image as gluon (0) or quark (1)\n",
    "        for filename in os.listdir(root_dir):\n",
    "            if filename.endswith(\".png\"):\n",
    "                label = 0 if \"type0\" in filename else 1   # Binary classification: 0 = gluon, 1 = quark\n",
    "                self.images.append(filename)\n",
    "                self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images) # Total number of images\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.images[idx]) # Construct full path to image\n",
    "        image = Image.open(img_path).convert(\"RGB\")  # The images are converted to RGB format\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image) # Apply transformations (resize, normalize, etc.)\n",
    "\n",
    "        label = self.labels[idx] # Get corresponding label\n",
    "        return image, label # Return processed image and its label\n",
    "    \n",
    "# Function to load the dataset and split it into training/validation and testing sets\n",
    "def load_jet_images(data_dir, img_size=299):\n",
    "    # Define preprocessing transformations for images\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)), # Resize image\n",
    "        transforms.ToTensor(), # Convert image to tensor\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # Normalize with ImageNet stats\n",
    "        #transforms.Lambda(lambda x: x.expand(3, -1, -1)),\n",
    "    ])\n",
    "\n",
    "    # Load the personalized dataset\n",
    "    full_dataset = JetImageDataset(root_dir=data_dir, transform=transform)\n",
    "    labels = np.array(full_dataset.labels) # Extract labels for stratification\n",
    "\n",
    "    # Stratification using sklearn train_test_split\n",
    "     # Create index array to split the dataset\n",
    "    indices = np.arange(len(full_dataset))\n",
    "\n",
    "    # Stratified split into training/validation and test sets\n",
    "    train_val_indices, test_indices = train_test_split(\n",
    "        indices,\n",
    "        test_size=0.2, # 80% train/val, 20% test\n",
    "        stratify=labels, # Preserve class distribution\n",
    "        random_state=42  # For reproducibility\n",
    "    )\n",
    "\n",
    "    # Create subsets for training/validation and testing\n",
    "    train_val_set = Subset(full_dataset, train_val_indices)\n",
    "    test_set = Subset(full_dataset, test_indices)\n",
    "\n",
    "    # Return both subsets\n",
    "    return train_val_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of a custom ensemble neural network combining ResNet50 and InceptionV3\n",
    "class ResNetModel(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "\n",
    "        # === ResNet50 Setup ===\n",
    "        # Load pretrained ResNet50 and replace its classifier head with a linear layer outputting 1024 features\n",
    "        self.resnet50 = models.resnet50(pretrained=True) \n",
    "        self.resnet50.fc = nn.Linear(self.resnet50.fc.in_features, 1024)\n",
    "\n",
    "        \n",
    "        # === Fusion + Classifier ===\n",
    "        #Apply a final classification head\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(1024 , 512), \n",
    "            nn.ReLU(),   # Non-linearity\n",
    "            nn.Dropout(0.8), # Regularization\n",
    "            nn.Linear(512, num_classes) # Output logits for each class\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ResNet feature extraction\n",
    "        resnet_out = self.resnet50(x)   # Shape: [batch_size, 1024]\n",
    "        output = self.fusion(resnet_out)  # Final prediction logits\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model for one epoch using a single GPU\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train() # Set model to training mode\n",
    "    total_loss = 0.0 # Accumulate training loss\n",
    "    correct = 0  # Count of correct predictions\n",
    "    total = 0   # Total number of processed samples\n",
    "\n",
    "    # Iterate over mini-batches\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device) # Move input data and labels to GPU\n",
    "\n",
    "        optimizer.zero_grad()  # Clear gradients from previous step\n",
    "        outputs = model(inputs) # Forward pass through ensemble model\n",
    "\n",
    "        loss = criterion(outputs, labels) # Compute main loss\n",
    "        preds = torch.argmax(outputs, dim=1)  # Get predicted class (highest score)\n",
    "\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        total_loss += loss.item() * inputs.size(0) # Accumulate weighted loss\n",
    "        correct += (preds == labels).sum().item()  # Count correct predictions\n",
    "        total += labels.size(0)       # Update total sample count\n",
    "    \n",
    "    # Compute average loss and accuracy over the entire epoch\n",
    "    avg_loss = total_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Evaluate the model on a validation set\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()  # Set model to evaluation mode (disables dropout, etc.)\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation, No gradients needed during evaluation (faster, less memory)\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)  # Forward pass through model \n",
    "            \n",
    "    \n",
    "            loss = criterion(outputs, labels) # Compute loss\n",
    "            preds = torch.argmax(outputs, dim=1) # Get predicted class (index of highest probability)\n",
    "\n",
    "            total_loss += loss.item() * inputs.size(0)  # Accumulate weighted loss\n",
    "            correct += (preds == labels).sum().item()  # Count correct predictions\n",
    "            total += labels.size(0)   # Update sample count\n",
    "    \n",
    "    # Compute average loss and accuracy\n",
    "    avg_loss = total_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_crossval_and_save_models(dataset, model_dir, device, k_folds=5, batch_size=64, max_epochs=15, patience=5):\n",
    "    # Create the directory to save models if it doesn't exist\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    #labels = np.array(dataset.dataset.labels) if isinstance(dataset, torch.utils.data.Subset) else np.array(dataset.labels)\n",
    "    base_indices = dataset.indices  # Get global indices from the original dataset inside the Subset object\n",
    "    labels = np.array([dataset.dataset.labels[i] for i in base_indices])  # Extract corresponding labels for those indices\n",
    "\n",
    "    # Define a stratified K-fold cross-validator to preserve class proportions\n",
    "    kfold = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)  \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Set the device to GPU if available, otherwise CPU\n",
    "    print(f\"\\nTraining divice: {device}\")\n",
    "    \n",
    "    # Print GPU details if CUDA is available\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"\\nGPUs availables: {torch.cuda.device_count()}\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    \n",
    "    # Initialize a final model instance and move to device\n",
    "    final_model = ResNetModel().to(device)\n",
    "    # If multiple GPUs are available, wrap model with DataParallel\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        final_model = nn.DataParallel(final_model)\n",
    "\n",
    "    # Lists to track metrics across all folds\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    # Start cross-validation loop\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(np.zeros(len(labels)), labels)):\n",
    "        print(f\"\\n Fold {fold+1}/{k_folds}\")\n",
    "        \n",
    "        # Create training DataLoader for the current fold\n",
    "        train_loader = DataLoader(\n",
    "            Subset(dataset, train_idx),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=min(4, os.cpu_count()),\n",
    "            pin_memory=True\n",
    "        )\n",
    "        #train_loader = DataLoader(Subset(dataset, train_idx), batch_size=batch_size, num_workers=min(4, os.cpu_count()),shuffle=True)\n",
    "        # Create validation DataLoader for the current fold\n",
    "        val_loader = DataLoader(\n",
    "            Subset(dataset, val_idx), \n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=min(4, os.cpu_count()),\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        model = ResNetModel() # Initialize a new model instance for this fold\n",
    "        # Wrap in DataParallel if using multiple GPUs\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            print(f\"Using {torch.cuda.device_count()} GPUs with DataParallel\")\n",
    "            model = nn.DataParallel(model)\n",
    "        model.to(device) # Move model to the selected device\n",
    "        \n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)  # Added weight decay for regularization\n",
    "        criterion = nn.CrossEntropyLoss()  # Define loss function\n",
    "        \n",
    "        # Variables to store best model weights and early stopping counter\n",
    "        best_loss = float(\"inf\")\n",
    "        best_wts = copy.deepcopy(model.state_dict())\n",
    "        no_improve = 0\n",
    "\n",
    "        # Begin training loop for the fold\n",
    "        for epoch in range(max_epochs):\n",
    "            # Train for one epoch and compute metrics\n",
    "            train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "            val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "\n",
    "            # Print training and validation stats\n",
    "            print(f'Epoch {epoch + 1}/{max_epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} '\n",
    "                  f'| Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}')\n",
    "            \n",
    "            # Check for improvement in validation loss\n",
    "\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                best_wts = copy.deepcopy(model.state_dict())\n",
    "                no_improve = 0  # Reset patience counter\n",
    "            else:\n",
    "                no_improve += 1   # Increment patience counter\n",
    "\n",
    "            # Trigger early stopping if no improvement\n",
    "            if no_improve >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "        \n",
    "        # Append final metrics for this fold\n",
    "        val_accuracies.append(val_acc)\n",
    "        train_accuracies.append(train_acc)\n",
    "\n",
    "        # Load best weights and save the model to disk\n",
    "        model.load_state_dict(best_wts)\n",
    "        save_path = os.path.join(model_dir, f\"fold_{fold+1}.pt\")\n",
    "        torch.save(model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict(), save_path)\n",
    "\n",
    "    # Compute and print average metrics across folds\n",
    "    avg_train_accuracy = np.mean(train_accuracies)\n",
    "    avg_val_accuracy = np.mean(val_accuracies)\n",
    "    #avg_test_accuracy = np.mean(test_accuracies)\n",
    "\n",
    "    print(f'\\nAverage Training Accuracy: {avg_train_accuracy:.4f}')\n",
    "    print(f'Average Validation Accuracy: {avg_val_accuracy:.4f}')\n",
    "    #print(f'Testing Accuracy: {test_accuracy:.4f}')\n",
    "    #print(f'Testing Score: {test_score:.4f}')\n",
    "\n",
    "    return avg_train_accuracy, avg_val_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory where the Jet images are stored\n",
    "data_dir = './data/Jet2Image_g-q_170k'  # Change this path if your dataset is in a different location\n",
    "\n",
    "\n",
    "# Select device: use GPU if available, otherwise fallback to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_gpus = torch.cuda.device_count() # Count the number of available GPUs\n",
    "\n",
    "# Load the training/validation and testing sets using the custom Jet image loader\n",
    "train_val_set, test_set = load_jet_images(data_dir)\n",
    "\n",
    "\n",
    "model_dir=\"5models_Resnet_gq_170k\" # Define directory where models trained for each fold will be saved\n",
    "\n",
    "# Run cross-validation training on the train/validation set and save the models\n",
    "Train_accuracy_avg, validation_accuracy_avg = run_crossval_and_save_models(train_val_set, model_dir, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_test(test_loader, model_dir, device):\n",
    "    all_probs = []  # List to collect predicted probabilities from each batch\n",
    "    all_labels = []  # List to collect true labels\n",
    "    ensemble_models = []  # List to store loaded models\n",
    "\n",
    "    # Load the 5 trained models saved from cross-validation\n",
    "    for i in range(5):\n",
    "        model_path = os.path.join(model_dir, f\"fold_{i+1}.pt\")  # Path for each fold model\n",
    "        model = ResNetModel(num_classes=2)  # Instantiate the same model architecture used during training\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device)) # Load trained weights\n",
    "        model.to(device)  # Move model to device (GPU or CPU)\n",
    "        model.eval()   # Set model to evaluation mode\n",
    "        ensemble_models.append(model)   # Store model for ensemble prediction\n",
    "\n",
    "    with torch.no_grad():   # Disable gradient computation for efficiency\n",
    "        for data, labels in test_loader:\n",
    "            data = data.to(device)    # Move input images to device\n",
    "            labels = labels.to(device)    # Move labels to device\n",
    "            all_preds = []    # Store predictions from all models for the current batch\n",
    "\n",
    "            for model in ensemble_models:\n",
    "                outputs = model(data) # Forward pass\n",
    "                probs = torch.softmax(outputs, dim=1)   # Apply softmax to get class probabilities\n",
    "                all_preds.append(probs)   # Store probabilities from this model\n",
    "\n",
    "            # Average predictions across all models\n",
    "            avg_probs = torch.stack(all_preds).mean(dim=0)\n",
    "            all_probs.append(avg_probs)   # Collect averaged predictions\n",
    "            all_labels.append(labels)     # Collect corresponding labels\n",
    "    \n",
    "    # Concatenate results across all batches\n",
    "    all_probs = torch.cat(all_probs)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    # Convert probability outputs to predicted class labels\n",
    "    preds = torch.argmax(all_probs, dim=1)\n",
    "\n",
    "    acc = (preds == all_labels).float().mean().item()  # Calculate accuracy\n",
    "    auc = roc_auc_score(all_labels.cpu().numpy(), all_probs[:, 1].cpu().numpy())   # Calculate AUC (Area Under the Curve)\n",
    "\n",
    "    return acc, auc, all_labels.cpu().numpy(), all_probs.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(y_true, y_probs, model_name):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_probs)  # Calculate False Positive Rate and True Positive Rate\n",
    "    roc_auc = auc(fpr, tpr)  # Compute the Area Under the Curve (AUC)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))  # Set the figure size for the plot\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'{model_name} (AUC = {roc_auc:.2f})')  # Plot the ROC curve\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')   # Plot the diagonal line (random guess line)\n",
    "    plt.xlim([0.0, 1.0])   # Limit x-axis to [0, 1]\n",
    "    plt.ylim([0.0, 1.05])   # Limit y-axis to [0, 1.05]\n",
    "    plt.xlabel('False Positive Rate')  # X-axis label\n",
    "    plt.ylabel('True Positive Rate')   # Y-axis label\n",
    "    plt.title(f'ROC Curve - {model_name}')  # Plot title with model name\n",
    "    plt.legend(loc=\"lower right\")   # Display legend in lower right\n",
    "    plt.show()\n",
    "    \n",
    "# Function to plot the histogram of predicted scores for both classes\n",
    "def plot_score_distributions(y_true, y_probs, model_name):\n",
    "    scores_class0 = y_probs[y_true == 0]  # Get predicted probabilities for class 0 (gluon)\n",
    "    scores_class1 = y_probs[y_true == 1]  # Get predicted probabilities for class 1 (z-boson)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.hist(scores_class0, bins=50, alpha=0.5, label='Gluon', color='blue') # Histogram for class 0\n",
    "    plt.hist(scores_class1, bins=50, alpha=0.5, label='quark', color='red') # Histogram for class 1\n",
    "    plt.xlabel('Classifier Score')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(f'Score Distributions - {model_name}')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory where the Jet images are stored\n",
    "#data_dir = './data/Jet2Image_g-t_170k'  # Change this path if your dataset is in a different location\n",
    "\n",
    "\n",
    "# Select device: use GPU if available, otherwise fallback to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_gpus = torch.cuda.device_count() # Count the number of available GPUs\n",
    "\n",
    "# Load the training/validation and testing sets using the custom Jet image loader\n",
    "#train_val_set, test_set = load_jet_images(data_dir)\n",
    "# Create DataLoader for the test set with a batch size of 32\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=32)\n",
    "model_dir=\"5models_Resnet_gq_170k\"  # Directory where the 5 trained models are saved\n",
    "\n",
    "# Run ensemble testing across the 5 saved models\n",
    "test_acc, test_auc, true_all_labels, test_all_probs = ensemble_test(\n",
    "    test_loader=test_loader,\n",
    "    model_dir = model_dir,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Print final test accuracy and AUC score\n",
    "print(f'\\nTesting Accuracy: {test_acc:.4f}')\n",
    "print(f'Testing Auc: {test_auc:.4f}')\n",
    "\n",
    "# Extract ground truth labels and predicted probabilities for class 1 (z-boson)\n",
    "y_true = np.array(true_all_labels)\n",
    "y_pred = np.array(test_all_probs)[:,1] # Use probabilities of the positive class (z-boson)\n",
    "\n",
    "# Generate ROC curve for the ensemble model\n",
    "plot_roc_curve(y_true, y_pred, \"Test ResNet g-q\")\n",
    "plot_score_distributions(y_true, y_pred, \"Test ResNet g-q\")   # Generate histogram of predicted scores for each class\n",
    "\n",
    "# Save the predictions and labels into a .npz file for later analysis or plotting\n",
    "model_name = \"ROC-Resnet-170k\"\n",
    "np.savez(f'{model_name}_gq.npz', y_true=y_true, y_pred=y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
