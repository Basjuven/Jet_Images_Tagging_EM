{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler, random_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch.optim as optim\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, Subset\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class JetImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Iterate through the directory to load and label the generated images\n",
    "        for filename in os.listdir(root_dir):\n",
    "            if filename.endswith(\".png\"):\n",
    "                label = 0 if \"type0\" in filename else 1  # 0 = gluon, 1 = quark\n",
    "                self.images.append(filename)\n",
    "                self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.images[idx])\n",
    "        image = Image.open(img_path).convert(\"RGB\")  # The images are converted to RGB format\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        return image, label\n",
    "    \n",
    "\n",
    "def load_jet_images(data_dir, img_size=299):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        #transforms.Lambda(lambda x: x.expand(3, -1, -1)),\n",
    "    ])\n",
    "\n",
    "    # Load the personalized dataset\n",
    "    full_dataset = JetImageDataset(root_dir=data_dir, transform=transform)\n",
    "    labels = np.array(full_dataset.labels)\n",
    "\n",
    "    # Stratification using sklearn train_test_split\n",
    "    indices = np.arange(len(full_dataset))\n",
    "\n",
    "    train_val_indices, test_indices = train_test_split(\n",
    "        indices,\n",
    "        test_size=0.2,\n",
    "        stratify=labels,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    train_val_set = Subset(full_dataset, train_val_indices)\n",
    "    test_set = Subset(full_dataset, test_indices)\n",
    "\n",
    "    return train_val_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "def plot_roc_curve(y_true, y_probs, model_name):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'{model_name} (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve - {model_name}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "def plot_score_distributions(y_true, y_probs, model_name):\n",
    "    scores_class0 = y_probs[y_true == 0]\n",
    "    scores_class1 = y_probs[y_true == 1]\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.hist(scores_class0, bins=50, alpha=0.5, label='Gluon', color='blue')\n",
    "    plt.hist(scores_class1, bins=50, alpha=0.5, label='Quark', color='red')\n",
    "    plt.xlabel('Classifier Score')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(f'Score Distributions - {model_name}')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Nuevo modelo incorporado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.resnet50 = torchvision.models.resnet50(pretrained=True)\n",
    "        self.inceptionv3 = torchvision.models.inception_v3(pretrained=True, aux_logits=True)\n",
    "        \n",
    "        # Modify the input layer of ResNet50\n",
    "        self.resnet50.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        \n",
    "        # Specific Configuration for InceptionV3:\n",
    "        # Disable automatic input transformation\n",
    "        self.inceptionv3.transform_input = False\n",
    "        # Modify the first layer to acept 3 chanels (sin tocar Conv2d_1a_3x3 directamente)\n",
    "        original_first_conv = self.inceptionv3.Conv2d_1a_3x3.conv\n",
    "        self.inceptionv3.Conv2d_1a_3x3.conv = nn.Conv2d(\n",
    "            in_channels=3,  # Changed to 3 canales\n",
    "            out_channels=original_first_conv.out_channels,\n",
    "            kernel_size=original_first_conv.kernel_size,\n",
    "            stride=original_first_conv.stride,\n",
    "            padding=original_first_conv.padding,\n",
    "            bias=False\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Average the weights across the RGB channels and replicate\n",
    "            weights_1ch = original_first_conv.weight.mean(dim=1, keepdim=True)\n",
    "            self.inceptionv3.Conv2d_1a_3x3.conv.weight = nn.Parameter(weights_1ch.repeat(1,3,1,1))\n",
    "       \n",
    "        \n",
    "        # Perform selective parameter freezing\n",
    "        for param in self.resnet50.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.inceptionv3.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Unfreeze the final layers\n",
    "        for param in list(self.resnet50.parameters())[-50:]:\n",
    "            param.requires_grad = True\n",
    "        for param in list(self.inceptionv3.parameters())[-30:]:\n",
    "            param.requires_grad = True\n",
    "\n",
    "        # Replace the fully connected layers\n",
    "        self.resnet50.fc = nn.Identity()  # Extrat 2048 features\n",
    "        self.inceptionv3.fc = nn.Identity()  # Extrat 2048 features\n",
    "        \n",
    "        # Attention mechanism for feature fusion\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(2048*2, 512),  # 2048 of ResNet + 2048 of Inception\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 2048*2),\n",
    "            nn.Sigmoid()  # Sigmoide activation atenttion (0-1)\n",
    "        )\n",
    "        \n",
    "        # Classifier layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(2048*2, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),  # Dropout alto para regularizaci√≥n\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 2)  # Salida binaria\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Process with both models\n",
    "        features_resnet = self.resnet50(x)\n",
    "        features_inception = self.inceptionv3(x)\n",
    "        \n",
    "        # InceptionV3 return a tuple (output, aux_output) if aux_logits=True\n",
    "        if isinstance(features_inception, tuple):\n",
    "            features_inception = features_inception[0]  # Take the principal output\n",
    "        \n",
    "        combined = torch.cat((features_resnet, features_inception), dim=1)\n",
    "        # Calculate attention weights (batch_size x 4096)\n",
    "        attention_weights = self.attention(combined)\n",
    "        \n",
    "        # Apply attention (multiplication element-wise)\n",
    "        attended_features = combined * attention_weights\n",
    "        \n",
    "        # final Clasification \n",
    "        #return self.classifier(attended_features)\n",
    "        out = self.classifier(attended_features)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### viejo modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        self.resnet50 = torchvision.models.resnet50(pretrained=True)\n",
    "        self.resnet50.fc = nn.Linear(2048, 2)  # Cambiar la salida a 2 clases\n",
    "\n",
    "        self.inceptionv3 = torchvision.models.inception_v3(pretrained=True, aux_logits=True)\n",
    "        self.inceptionv3.fc = nn.Linear(2048, 2)  # Cambiar la salida a 2 clases\n",
    "\n",
    "        # La capa final de combinaci√≥n debe recibir el tama√±o correcto de la concatenaci√≥n\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2, 128),  # Solo las salidas de ResNet y Inception\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 2)  # Cambiar a dos salidas para clasificaci√≥n binaria\n",
    "        )\n",
    "        #self.transform_resnet = transforms.Resize((224, 224))\n",
    "        #self.transform_inception = transforms.Resize((299, 299))    \n",
    "\n",
    "    def forward(self, x):\n",
    "        features_resnet = self.resnet50(x)\n",
    "        features_inception = self.inceptionv3(x)\n",
    "        #resnet_input = self.transform_resnet(x)\n",
    "        #inception_input = self.transform_inception(x)\n",
    "\n",
    "        # Procesar cada modelo individualmente\n",
    "        #resnet_out = self.resnet50(resnet_input)\n",
    "        \n",
    "        # Manejar salida de InceptionV3 para evitar el error\n",
    "        inception_outputs = self.inceptionv3(inception_input)\n",
    "        inception_out = inception_outputs[0] if isinstance(inception_outputs, tuple) else inception_outputs\n",
    "        \n",
    "        # Concatenar salidas y calcular la salida combinada\n",
    "        #combined = torch.cat((resnet_out, inception_out), dim=1)\n",
    "        out = self.fc(inception_out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    all_labels = []\n",
    "    incept_preds = []\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "        total_train += labels.size(0)\n",
    "\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        incept_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "        \n",
    "\n",
    "    train_accuracy = correct_train / total_train\n",
    "    return running_loss / len(train_loader), train_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model_individuals(model, val_loader, criterion, device, target_tpr=0.5):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    all_labels = []\n",
    "    incept_preds = []\n",
    "    incept_confidences = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            # Obt√©n la confianza de cada modelo\n",
    "            #incept_confidences.append(softmax(outputs, dim=1).cpu().numpy())\n",
    "            # Manejar salida de InceptionV3\n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs = outputs[0]  # Tomar la salida principal\n",
    "            \n",
    "            # Calcular probabilidades con softmax y guardar confianzas de la CLASE POSITIVA (√≠ndice 1)\n",
    "            confidences = softmax(outputs, dim=1).cpu().numpy()\n",
    "            incept_confidences.append(confidences[:, 1])  # Cambio clave: usar solo clase positiva\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "            total_val += labels.size(0)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            incept_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "            \n",
    "    val_accuracy = correct_val / total_val\n",
    "    #cm_incept = confusion_matrix(all_labels, incept_preds)\n",
    "\n",
    "    predicted_probs = np.concatenate(incept_confidences)  # Ya son solo las de clase positiva\n",
    "\n",
    "\n",
    "\n",
    "    return running_loss / len(val_loader), val_accuracy, all_labels, predicted_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(models, test_loader, device):\n",
    "    # Set all models to evaluation mode\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = []\n",
    "\n",
    "            for model in models:\n",
    "                output = model(images)\n",
    "                output = torch.softmax(output, dim=1)  # convert logits to probabilities\n",
    "                outputs.append(output)\n",
    "\n",
    "            # Stack outputs and average them across models\n",
    "            outputs = torch.stack(outputs)  # shape: (n_models, batch_size, n_classes)\n",
    "            avg_output = torch.mean(outputs, dim=0)  # shape: (batch_size, n_classes)\n",
    "\n",
    "            all_probs.append(avg_output.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    # Concatenate all batches\n",
    "    all_probs = torch.cat(all_probs, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "\n",
    "    # Predicted classes\n",
    "    _, predicted = torch.max(all_probs, 1)\n",
    "\n",
    "    # Accuracy\n",
    "    correct = (predicted == all_labels).sum().item()\n",
    "    total = all_labels.size(0)\n",
    "    accuracy = correct / total\n",
    "\n",
    "    # AUC Score (only if binary classification)\n",
    "    try:\n",
    "        auc_score = roc_auc_score(all_labels.numpy(), all_probs[:, 1].numpy())\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not compute AUC Score. Reason: {e}\")\n",
    "        auc_score = None\n",
    "\n",
    "    return accuracy, auc_score, all_labels.numpy().tolist(), all_probs.numpy().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_ensemble(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass con el ensemble completo\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Handling outputs (for InceptionV3 in the ensemble)\n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs = outputs[0]  # We take the main output\n",
    "            \n",
    "            # Loss and metrics calculation.\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Obtaining predictions\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            # Storage for metrics\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_probs.extend(probs[:, 1].cpu().numpy())  # Positive class probability.\n",
    "            \n",
    "    # Final metrics calculation\n",
    "    val_loss = running_loss / len(val_loader)\n",
    "    val_accuracy = correct / total\n",
    "    val_auc = roc_auc_score(all_labels, all_probs)\n",
    "    \n",
    "    return val_loss, val_accuracy, val_auc, all_labels, all_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import time\n",
    "def cross_validate_ensemble(train_val_set, test_set, k_folds=5, epochs=30, batch_size=64):\n",
    "    # Obtain labels for stratification\n",
    "    labels = [train_val_set[i][1] for i in range(len(train_val_set))]\n",
    "    \n",
    "    # Use StratifiedKFold to maintain class proportion\n",
    "    kfold = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\nTraining divice: {device}\") \n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"\\nGPUs availables: {torch.cuda.device_count()}\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "    final_model = EnsembleModel().to(device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        final_model = nn.DataParallel(final_model)\n",
    "    \n",
    "    # Metrics list\n",
    "    fold_metrics = {\n",
    "        'train_acc': [],\n",
    "        'val_acc': [],\n",
    "        'models': []  # To save the models of each fold\n",
    "    }\n",
    "\n",
    "    # Training setup\n",
    "    train_accuracies, val_accuracies, test_accuracy = [], [], []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    best_models = []  # to save the best model in each fold\n",
    "\n",
    "    # Early stopping config\n",
    "    patience = 5\n",
    "    min_delta = 0.005\n",
    "\n",
    "    for fold, (train_ids, val_ids) in enumerate(kfold.split(range(len(train_val_set)), labels)):\n",
    "        print(f'\\n{\"=\"*50}\\nTraining on Fold {fold + 1}/{k_folds}\\n{\"=\"*50}')\n",
    "        \n",
    "        # DataLoaders optimized\n",
    "        train_loader = DataLoader(\n",
    "            train_val_set,\n",
    "            batch_size=batch_size,\n",
    "            sampler=SubsetRandomSampler(train_ids),\n",
    "            num_workers=min(4, os.cpu_count()),\n",
    "            pin_memory=True,\n",
    "            persistent_workers=True\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            train_val_set,\n",
    "            batch_size=batch_size,\n",
    "            sampler=SubsetRandomSampler(val_ids),\n",
    "            num_workers=min(4, os.cpu_count()),\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        model = EnsembleModel()\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            print(f\"Using {torch.cuda.device_count()} GPUs with DataParallel\")\n",
    "            model = nn.DataParallel(model)\n",
    "        model.to(device)\n",
    " \n",
    "        optimizer = optim.AdamW([\n",
    "            {'params': [p for p in model.module.resnet50.parameters() if p.requires_grad], 'lr': 1e-5},\n",
    "            {'params': [p for p in model.module.inceptionv3.parameters() if p.requires_grad], 'lr': 1e-5},\n",
    "            {'params': model.module.attention.parameters(), 'lr': 3e-4},\n",
    "            {'params': model.module.classifier.parameters(), 'lr': 1e-3}\n",
    "        ], weight_decay=1e-4)\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, \n",
    "            mode='max', \n",
    "            factor=0.5, \n",
    "            patience=5, \n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "        # Loss function with label smoothing\n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "        \n",
    "        # Early stopping variables\n",
    "        best_val_acc = 0.0\n",
    "        early_stop_counter = 0\n",
    "        best_model_state = None\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Training\n",
    "            train_loss, train_acc = train_model(model, train_loader, criterion, optimizer, device)\n",
    "            \n",
    "            # Validation\n",
    "            val_loss, val_acc, val_auc, val_labels, val_probs = validate_ensemble(\n",
    "                model, val_loader, criterion, device\n",
    "            )\n",
    "\n",
    "            # Update scheduler based in val_acc\n",
    "            scheduler.step(val_acc)\n",
    "            \n",
    "            # Calculate epochs time\n",
    "            epoch_time = time.time() - start_time\n",
    "            \n",
    "            print(f'Epoch {epoch + 1}/{epochs} | Time: {epoch_time:.2f}s')\n",
    "            print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}')\n",
    "            print(f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}')\n",
    "            \n",
    "            # Early stopping logic\n",
    "            if val_acc > best_val_acc + min_delta:\n",
    "                best_val_acc = val_acc\n",
    "                early_stop_counter = 0\n",
    "                best_model_state = deepcopy(model.state_dict())\n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "                if early_stop_counter >= patience:\n",
    "                    print(f'Early stopping triggered at epoch {epoch + 1}!')\n",
    "                    model.load_state_dict(best_model_state)\n",
    "                    break\n",
    "\n",
    "        # Save results from each fold\n",
    "        fold_metrics['train_acc'].append(train_acc)\n",
    "        fold_metrics['val_acc'].append(val_acc)\n",
    "        fold_metrics['models'].append(best_model_state)\n",
    "        # Save best fold model\n",
    "\n",
    "        #best_models.append(deepcopy(model.state_dict()))\n",
    "        \n",
    "        train_accuracies.append(train_acc)\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        all_labels.extend(val_labels)\n",
    "        all_probs.extend(val_probs)\n",
    "        \n",
    "        print(f'\\nFold {fold + 1} Results:')\n",
    "        print(f'Best Val Acc: {best_val_acc:.4f}')\n",
    "\n",
    "\n",
    "    y_true = np.array(all_labels)\n",
    "    y_pred = np.array(all_probs)\n",
    "    # Generar gr√°ficos con TODOS los datos de validaci√≥n\n",
    "    #plot_roc_curve(y_true, y_pred, \"Inception-V3\")\n",
    "    plot_roc_curve(y_true, y_pred, \"Val Ensemble\")\n",
    "    #plot_score_distributions(y_true, y_pred, \"Inception-V3\")\n",
    "    plot_score_distributions(y_true, y_pred, \"Val Ensemble\")\n",
    "        \n",
    "    test_loader = DataLoader(\n",
    "            test_set,\n",
    "            batch_size=batch_size*2,\n",
    "            shuffle=False,\n",
    "            num_workers=min(4, os.cpu_count()),\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "    # Evaluaci√≥n en test set\n",
    "    test_acc, test_auc, true_labels, test_probs = test_model(fold_metrics['models'], test_loader, device)\n",
    "            \n",
    "    test_accuracy.append(test_acc)\n",
    "    # Resultados finales\n",
    "    avg_train_acc = np.mean(train_accuracies)\n",
    "    avg_val_acc = np.mean(val_accuracies)\n",
    "    #avg_test_acc = np.mean(test_accuracies)\n",
    "    \n",
    "    print(f'\\n{\"=\"*50}')\n",
    "    print(f'Cross-Validation Complete')\n",
    "    print(f'Average Training Accuracy: {avg_train_acc:.4f}')\n",
    "    print(f'Average Validation Accuracy: {avg_val_acc:.4f}')\n",
    "    print(f'Average Testing Accuracy: {test_accuracy:.4f}')\n",
    "    print(f'{\"=\"*50}')\n",
    "    \n",
    "    return avg_train_acc, avg_val_acc, test_accuracy, all_labels, all_probs, test_auc, true_labels, test_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import time\n",
    "# Validaci√≥n cruzada del modelo de ensamble sin ViT\n",
    "def cross_validate_ensemble(train_val_set, test_set, k_folds=10, epochs=30, batch_size=256):\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"\\nGPUs disponibles: {torch.cuda.device_count()}\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "    train_accuracies, val_accuracies, test_accuracies = [], [], []\n",
    "    CM_incept = []\n",
    "    incept_rejection_rates = []\n",
    "    \n",
    "    # Listas para almacenar todas las probabilidades y etiquetas de validaci√≥n\n",
    "    all_labels = []\n",
    "    all_probs = []  # Ahora almacenar√° SOLO las probabilidades de la clase positiva\n",
    "\n",
    "    patience = 2  # N√∫mero de √©pocas sin mejora antes de parar\n",
    "    min_delta = 0.001  # Cambio m√≠nimo para considerar mejora\n",
    "    best_val_loss = float('inf')\n",
    "    early_stop_counter = 0\n",
    "\n",
    "    for fold, (train_ids, val_ids) in enumerate(kfold.split(train_val_set)):\n",
    "        #print(f'\\nTraining on Fold {fold + 1}...')\n",
    "        print(f'\\n{\"=\"*50}\\nFold {fold + 1}/{k_folds}\\n{\"=\"*50}')\n",
    "        \n",
    "        # DataLoaders optimizados\n",
    "        train_loader = DataLoader(\n",
    "            train_val_set,\n",
    "            batch_size=batch_size,\n",
    "            sampler=SubsetRandomSampler(train_ids),\n",
    "            num_workers=min(4, os.cpu_count()),\n",
    "            pin_memory=True,\n",
    "            persistent_workers=True\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            train_val_set,\n",
    "            batch_size=batch_size*2,  # Mayor batch_size para validaci√≥n\n",
    "            sampler=SubsetRandomSampler(val_ids),\n",
    "            num_workers=min(4, os.cpu_count()),\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        test_loader = DataLoader(\n",
    "            test_set,\n",
    "            batch_size=batch_size*2,\n",
    "            shuffle=False,\n",
    "            num_workers=min(4, os.cpu_count()),\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        # Modelo y optimizador\n",
    "        model = EnsembleModel()\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            print(f\"Usando {torch.cuda.device_count()} GPUs con DataParallel\")\n",
    "            model = nn.DataParallel(model)\n",
    "        model.to(device)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        # Reemplazar tu optimizer actual por:\n",
    "        optimizer = optim.AdamW([\n",
    "            {'params': [p for p in model.resnet50.parameters() if p.requires_grad], 'lr': 1e-5},\n",
    "            {'params': [p for p in model.inceptionv3.parameters() if p.requires_grad], 'lr': 1e-5},\n",
    "            {'params': model.attention.parameters(), 'lr': 3e-4},\n",
    "            {'params': model.classifier.parameters(), 'lr': 1e-3}\n",
    "        ], weight_decay=1e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, \n",
    "            mode='max', \n",
    "            factor=0.5, \n",
    "            patience=3, \n",
    "            verbose=True\n",
    "        )\n",
    "        #optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "        #scaler = GradScaler()  # Para mixed-precision\n",
    "        \n",
    "        # Early Stopping variables\n",
    "        best_val_loss = float('inf')\n",
    "        early_stop_counter = 0\n",
    "        best_model_state = None\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            start_time = time.time()\n",
    "            epoch_time = time.time() - start_time\n",
    "            train_loss, train_acc = train_model(model, train_loader, criterion, optimizer, device)\n",
    "\n",
    "            val_loss, val_acc, val_labels, val_probs = validate_model_individuals(model, val_loader, criterion, device)\n",
    "            \n",
    "            print(f'Epoch {epoch + 1}/{epochs} |  Time: {epoch_time:.2f}s | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} '\n",
    "                  f'| Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}')\n",
    "            #print(f'Epoch {epoch + 1}/{epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}')\n",
    "            \n",
    "\n",
    "            # Early Stopping Logic\n",
    "            if val_loss < best_val_loss - min_delta:\n",
    "                best_val_loss = val_loss\n",
    "                early_stop_counter = 0\n",
    "                best_model_weights = model.state_dict().copy()  # Guardar mejor modelo\n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "                if early_stop_counter >= patience:\n",
    "                    print(f'Early stopping triggered at epoch {epoch + 1}!')\n",
    "                    model.load_state_dict(best_model_weights)  # Restaurar mejor modelo\n",
    "                    break\n",
    "\n",
    "\n",
    "        #incept_rejection_rates.append(incept_rejection_rate)\n",
    "        #CM_incept.append(val_cm_incept)\n",
    "        \n",
    "        # Almacenar etiquetas y probabilidades CORRECTAS (clase positiva)\n",
    "        all_labels.extend(val_labels)\n",
    "        all_probs.extend(val_probs)  # val_probs ahora son las probabilidades de la clase 1\n",
    "        scheduler.step(val_acc)\n",
    "\n",
    "        val_accuracies.append(val_acc)\n",
    "        train_accuracies.append(train_acc)\n",
    "\n",
    "        # Evaluar y obtener tasa de rechazo del modelo de ensamble en el test set\n",
    "        test_acc, test_auc, true_all_labels, test_all_probs = test_model(model, test_loader, device)\n",
    "        test_accuracies.append(test_acc)\n",
    "\n",
    "    avg_train_accuracy = np.mean(train_accuracies)\n",
    "    avg_val_accuracy = np.mean(val_accuracies)\n",
    "    avg_test_accuracy = np.mean(test_accuracies)\n",
    "\n",
    "    # Promedio de matrices de confusi√≥n (suma en lugar de promedio)\n",
    "    #avg_cm_incept = np.sum(CM_incept, axis=0)  # M√°s informativo que el promedio\n",
    "\n",
    "    #avg_incept_rejection = np.mean(incept_rejection_rates)\n",
    "    \n",
    "\n",
    "    print(f'\\nAverage Training Accuracy: {avg_train_accuracy:.4f}')\n",
    "    print(f'Average Validation Accuracy: {avg_val_accuracy:.4f}')\n",
    "    print(f'Average Testing Accuracy: {avg_test_accuracy:.4f}')\n",
    "\n",
    "\n",
    "    return avg_train_accuracy, avg_val_accuracy, avg_test_accuracy, all_labels, all_probs, test_auc, true_all_labels, test_all_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dispositivo de entrenamiento: cuda\n",
      "\n",
      "GPUs disponibles: 2\n",
      "GPU 0: NVIDIA A100-SXM4-80GB\n",
      "GPU 1: NVIDIA A100-SXM4-80GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home02/jbassa/anaconda3/envs/scikit-hep/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/n/home02/jbassa/anaconda3/envs/scikit-hep/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/n/home02/jbassa/anaconda3/envs/scikit-hep/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Training on Fold 1/5\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home02/jbassa/anaconda3/envs/scikit-hep/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/n/home02/jbassa/anaconda3/envs/scikit-hep/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/n/home02/jbassa/anaconda3/envs/scikit-hep/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando 2 GPUs con DataParallel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home02/jbassa/anaconda3/envs/scikit-hep/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data_dir = './data/Jet2Image_g-q_170k'  # Cambia esto seg√∫n la ubicaci√≥n de tus im√°genes\n",
    "\n",
    "# Cargando el dataset con la funci√≥n corregida\n",
    "train_val_set, test_set = load_jet_images(data_dir)\n",
    "#train_val_set, test_set = load_cifar10()\n",
    "Train_accuracy_avg, validation_accuracy_avg, Testing_avg_acc, all_labels, all_probs, test_auc, true_all_labels, test_all_probs  = cross_validate_ensemble(train_val_set, test_set)\n",
    "\n",
    "y_true = np.array(true_all_labels)\n",
    "y_pred = np.array(test_all_probs)\n",
    "# Generar gr√°ficos con TODOS los datos de validaci√≥n\n",
    "#plot_roc_curve(y_true, y_pred, \"Inception-V3\")\n",
    "plot_roc_curve(y_true, y_pred, \"Test Ensemble\")\n",
    "#plot_score_distributions(y_true, y_pred, \"Inception-V3\")\n",
    "plot_score_distributions(y_true, y_pred, \"Test Ensemble\")\n",
    "# Ejemplo: Supongamos que tienes esto despu√©s de evaluar tu modelo\n",
    "#model_name = \"inception-v3\"  # Cambia esto seg√∫n corresponda\n",
    "model_name = \"ensemble\"\n",
    "np.savez(f'{model_name}_results_g-q.npz', y_true=y_true, y_pred=y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scikit-hep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
